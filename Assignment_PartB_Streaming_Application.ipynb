{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22818373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python3 in /opt/conda/lib/python3.8/site-packages (3.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install kafka-python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd49286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.8/site-packages (3.3.0)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317129 sha256=6989d6f05c8a47ac53bf1e080713984736d8b033cddc25422d08abe84a9c21fe\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/27/3e/a7/888155c6a7f230b13a394f4999b90fdfaed00596c68d3de307\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.5\n",
      "    Uninstalling py4j-0.10.9.5:\n",
      "      Successfully uninstalled py4j-0.10.9.5\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.3.0\n",
      "    Uninstalling pyspark-3.3.0:\n",
      "      Successfully uninstalled pyspark-3.3.0\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8064a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeohash\n",
      "  Downloading pygeohash-1.2.0.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pygeohash\n",
      "  Building wheel for pygeohash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pygeohash: filename=pygeohash-1.2.0-py2.py3-none-any.whl size=6152 sha256=84a3a31f6316e94c29b4a27c792dac36de6467377213dfbd5236a997d85cdcb4\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/70/98/c5/332f0986813a345d8869d98d134e5c89e322399d5450b1b05b\n",
      "Successfully built pygeohash\n",
      "Installing collected packages: pygeohash\n",
      "Successfully installed pygeohash-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b71121be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_201/206431194.py\", line 212, in <module>\n",
      "    query.awaitTermination()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py\", line 107, in awaitTermination\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1320, in __call__\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pprint\n",
    "import pygeohash as pgh\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "from pyspark.sql.functions import avg\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "def geohash_handler(latitude, longitude):\n",
    "    return pgh.encode(latitude, longitude, precision=3)\n",
    "\n",
    "def hotspots_handler(hotspots_aqua, hotspots_terra):\n",
    "    # If there are no hotspots from either satellite, return empty array.\n",
    "    if len(hotspots_aqua) == 0 and len(hotspots_terra) == 0:\n",
    "        return []\n",
    "    # If there are only hotspots from a single satellite, then there is no opportunity for a location\n",
    "    # to be close to another satellite, so return only the satellite that contains data.\n",
    "    elif len(hotspots_aqua) > 0 and len(hotspots_terra) == 0:\n",
    "        return hotspots_aqua\n",
    "    elif len(hotspots_aqua) == 0 and len(hotspots_terra) > 0:\n",
    "        return hotspots_terra\n",
    "    else:\n",
    "        hotspots = []\n",
    "\n",
    "        for aqua in hotspots_aqua:\n",
    "            count = 0\n",
    "\n",
    "            # Loop through both arrays to see if there's a location that is close by.\n",
    "            # If an aqua satellite location is close to a terra satellite location, take the average for confidence and surface_temp,\n",
    "            # then append the new hotspot data to the hotspots final array and remove the terra satellite data from the hotspots_terra array.\n",
    "            # So there are no duplicate matches in the future (the task was only to match two satellite locations).\n",
    "            while count < len(hotspots_terra):\n",
    "                terra = hotspots_terra[count]\n",
    "                if aqua['geo_hash'] == terra['geo_hash']:\n",
    "                    avg_hotspot = aqua\n",
    "                    avg_hotspot['confidence'] = (aqua['confidence'] + terra['confidence']) / 2\n",
    "                    avg_hotspot['surface_temperature_celsius'] = (aqua['surface_temperature_celsius'] + terra['surface_temperature_celsius']) / 2\n",
    "                    hotspots_terra.pop(count)\n",
    "                    hotspots.append(avg_hotspot)\n",
    "                    break\n",
    "                else:\n",
    "                    # If no close satellites, append aqua to the final array.\n",
    "                    hotspots.append(aqua)\n",
    "                count += 1\n",
    "\n",
    "        # If there are terra satellites that haven't been removed, append them to the final array.\n",
    "        if len(hotspots_terra) > 0:\n",
    "            for terra in hotspots_terra:\n",
    "                hotspots.append(terra)\n",
    "\n",
    "        return hotspots\n",
    "\n",
    "def climate_handler(climate, hotspots):\n",
    "    # If there was no climate data, return an empty dictionary.\n",
    "    if len(hotspots) > 0 and climate != {}:\n",
    "        for hotspot in hotspots:\n",
    "            # Check if climate and hotspot are close.\n",
    "            if climate['geo_hash'] == hotspot['geo_hash']:\n",
    "                # Check if natural or other.\n",
    "                if climate['air_temperature_celsius'] > 20 and climate['ghi'] > 180:\n",
    "                    hotspot['cause'] = 'natural'\n",
    "                else:\n",
    "                    hotspot['cause'] = 'other'\n",
    "\n",
    "                if 'hotspots' in climate:\n",
    "                    # Append hotspot.\n",
    "                    climate['hotspots'].append(hotspot)\n",
    "                else:\n",
    "                    climate['hotspots'] = [hotspot]\n",
    "\n",
    "    climate['station'] = 948700  # Station number required for DB data model.\n",
    "\n",
    "    return climate\n",
    "\n",
    "def stream_handler(batch_df):\n",
    "    hotspots_aqua = []\n",
    "    hotspots_terra = []\n",
    "    climate = {}\n",
    "\n",
    "    # Collect data from the streaming DataFrame batch and process it.\n",
    "    data_batch = batch_df.collect()\n",
    "\n",
    "    for data_row in data_batch:\n",
    "        data = data_row.value\n",
    "        # Calculate and set the geohash.\n",
    "        data['geo_hash'] = geohash_handler(data['latitude'], data['longitude'])\n",
    "        producer_id = data['producer_id']\n",
    "\n",
    "        if producer_id == 'producer_climate':\n",
    "            climate = data\n",
    "        elif producer_id == 'producer_hotspot_aqua':\n",
    "            hotspots_aqua.append(data)\n",
    "        elif producer_id == 'producer_hotspot_terra':\n",
    "            hotspots_terra.append(data)\n",
    "\n",
    "    # Analyze hotspot data, find if any are close by and merge them.\n",
    "    hotspots = hotspots_handler(hotspots_aqua, hotspots_terra)\n",
    "    # Merge hotspots with climate (depending if close and label if natural or other).\n",
    "    climate = climate_handler(climate, hotspots)\n",
    "\n",
    "    return climate\n",
    "\n",
    "def prepare_for_db(data):\n",
    "    # Create a new document dictionary (final version for DB) and clean up variables.\n",
    "    document = {}\n",
    "\n",
    "    document['date'] = datetime.datetime.fromisoformat(data['created_date'])\n",
    "    document['station'] = data['station']\n",
    "    document[\"air_temperature_celsius\"] = data['air_temperature_celsius']\n",
    "    document['relative_humidity'] = data['relative_humidity']\n",
    "    document['windspeed_knots'] = data['windspeed_knots']\n",
    "    document['max_wind_speed'] = data['max_wind_speed']\n",
    "    document['precipitation'] = data['precipitation']\n",
    "    document['precipitation_type'] = data['precipitation_type']\n",
    "    document['ghi'] = data['ghi']\n",
    "\n",
    "    if 'hotspots' in data:\n",
    "        document['hotspots'] = []\n",
    "        for hotspot in data['hotspots']:\n",
    "            hotspot_doc = {}\n",
    "            hotspot_doc['time'] = datetime.datetime.fromisoformat(hotspot['created_time'])\n",
    "            hotspot_doc['cause'] = hotspot['cause']\n",
    "            hotspot_doc['confidence'] = hotspot['confidence']\n",
    "            hotspot_doc['latitude'] = hotspot['latitude']\n",
    "            hotspot_doc['longitude'] = hotspot['longitude']\n",
    "            hotspot_doc['surface_temperature_celsius'] = hotspot['surface_temperature_celsius']\n",
    "            document['hotspots'].append(hotspot_doc)\n",
    "\n",
    "    return document\n",
    "\n",
    "def send_data_to_db(batch_df, batch_id):\n",
    "    # Collect data from the streaming DataFrame batch.\n",
    "    data_batch = batch_df.collect()\n",
    "\n",
    "    # Sometimes batches may have no data, so we ensure that it isn't saved to the database.\n",
    "    if len(data_batch) > 0:\n",
    "        # Send data to be transformed and analyzed.\n",
    "        climate_data = stream_handler(data_batch[0])  # Assuming one document per batch.\n",
    "\n",
    "        if len(climate_data) > 1:\n",
    "            # Send to remove key values that aren't in the data model, such as 'geo_hash'.\n",
    "            database_data = prepare_for_db(climate_data)\n",
    "\n",
    "            client = MongoClient()\n",
    "            db = client.fit3182_assignment_db\n",
    "            collection = db.climate\n",
    "\n",
    "            # Insert climate data into the database.\n",
    "            collection.insert_one(database_data)\n",
    "            pprint.pprint(database_data)\n",
    "\n",
    "            client.close()\n",
    "\n",
    "batch_interval = 10\n",
    "topic = \"Climate,Hotspot_AQUA,Hotspot_TERRA\"\n",
    "checkpoint_dir = \"/tmp/spark_streaming_checkpoint\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master('local[*]')\n",
    "         .appName(\"KafkaStreamProcessor\")\n",
    "         .getOrCreate())\n",
    "host_ip = \"192.168.1.5\"\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Define the schema for the data received from Kafka.\n",
    "schema = StructType([\n",
    "    StructField(\"created_date\", StringType()),\n",
    "    StructField(\"station\", IntegerType()),\n",
    "    StructField(\"producer_id\", StringType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"air_temperature_celsius\", IntegerType()),\n",
    "    StructField(\"relative_humidity\", FloatType()),\n",
    "    StructField(\"windspeed_knots\", FloatType()),\n",
    "    StructField(\"max_wind_speed\", FloatType()),\n",
    "    StructField(\"precipitation\", FloatType()),\n",
    "    StructField(\"precipitation_type\", StringType()),\n",
    "    StructField(\"ghi\", FloatType()),\n",
    "    StructField(\"confidence\", IntegerType()),\n",
    "    StructField(\"surface_temperature_celsius\", IntegerType()),\n",
    "])\n",
    "\n",
    "# Parse the JSON data and apply the defined schema to create a DataFrame with structured data.\n",
    "parsed_df = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Create a processing query that triggers the processing of each batch of data in 10-second intervals.\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(send_data_to_db) \\\n",
    "    .trigger(processingTime=f\"{batch_interval} seconds\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Clean up the checkpoint directory after the query finishes.\n",
    "shutil.rmtree(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cd9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88b06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
